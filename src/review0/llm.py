from langchain_ollama import OllamaLLM, ChatOllama, OllamaEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


def load_model(model_name="mistral", temperature=0.7, num_predict = 256, num_ctx=2048, is_chat=True):
    """ Loads a language model with the specified configuration.

        Args:
            model_name (str, optional): The name of the model to load. Defaults to "mistral".
            temperature (float, optional): Sampling temperature for the model. A lower value makes the output more deterministic. Defaults to 0.
            num_predict (int, optional): The maximum number of tokens to predict. Defaults to 256.
            num_ctx (int, optional): The context window size for the model. Defaults to 2048.
            is_chat (bool, optional): Whether to load the model in chat mode or standard mode. Defaults to True.

        Returns:
            object: An instance of the loaded language model, either in chat mode or standard mode.
    """
    if is_chat:
        llm = ChatOllama(model=model_name, temperature=temperature, num_predict = num_predict, num_ctx=num_ctx) # format="json")
    else:
        llm = OllamaLLM(model=model_name, temperature=temperature, num_predict = num_predict, num_ctx=num_ctx)
    return llm


def query(llm, prompt):
    """ Sends a prompt to the language model and retrieves the response.

        Args:
            llm (object): The loaded language model instance.
            prompt (str): The input prompt to query the model.

        Returns:
            str: The response generated by the language model.
    """
    response = llm.invoke(prompt)
    return response


def load_vectorstore(model_name, list_of_ids, list_of_texts, list_of_tags=None, chunk_size=1000, chunk_overlap=200):
    """ Creates and populates an in-memory vector store with embeddings and document chunks.

        Args:
            model_name (str): The name of the model to use for generating embeddings.
            list_of_ids (list): A list of document IDs.
            list_of_texts (list): A list of document contents.
            list_of_tags (list, optional): A list of tags corresponding to the documents. Defaults to None.

        Returns:
            InMemoryVectorStore: The populated vector store instance.
    """
    # Create vector store
    embeddings = OllamaEmbeddings(model=model_name)
    vector_store = InMemoryVectorStore(embeddings)
    if list_of_tags is  None:
        list_of_tags = ["default"] * len(list_of_texts)
    docs = [ Document(id=doc_id, page_content=content, metadata={"tag": tag}) 
        for doc_id, content, tag in zip (list_of_ids, list_of_texts, list_of_tags) 
    ]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_splits = text_splitter.split_documents(docs)
    # Index chunks
    _ = vector_store.add_documents(documents=all_splits)
    return vector_store


def get_qarag_prompt(model_name, vector_store, question):
    """ Generates a custom prompt for Question Answering with Retrieval-Augmented Generation (RAG).

        Args:
            model_name (str): The name of the model to use for generating the prompt.
            vector_store (InMemoryVectorStore): The vector store containing document embeddings.
            question (str): The question to be answered.

        Returns:
            str: The generated prompt with retrieved context and question.
    """
    #Â Custom prompt for RAG
    template = """{bos_str} {bins_str} Use the following pieces of context to answer the question at the end.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Use three sentences maximum and keep the answer as concise as possible.
    Always include the original sources of the text you derived the answer from at the end of the answer.
    {eins_str} {eos_str} {context}
    {bins_str}  Question: {question}
    Answer: {eins_str}"""

    bos_str, eos_str = "", ""
    bins_str, eins_str = "", ""
    if model_name.startswith("mistral"):
        #prompt = hub.pull("rlm/rag-prompt-mistral")
        bos_str, eos_str = "<s>", "</s>"
        bins_str, eins_str = "[INST]", "[/INST]"
    #else: 
    #    prompt = hub.pull("rlm/rag-prompt")
    prompt = PromptTemplate.from_template(template)
    retrieved_docs = vector_store.similarity_search(question)
    docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
    prompt = prompt.invoke({"question": question, "context": docs_content, "bos_str": bos_str, "eos_str": eos_str, "bins_str": bins_str, "eins_str": eins_str})
    return prompt

